\documentclass[runningheads]{llncs}

%---- Sonderzeichen-------%
\usepackage {ngerman}
%---- Codierung----%
\usepackage[latin1]{inputenc}	% for Unix and Windows
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{url}
\usepackage{llncsdoc}
%----- Mathematischer Zeichenvorrat---%
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
% fuer die aktuelle Zeit
\usepackage{scrtime}
\usepackage{listings}
\usepackage{subfigure}
\usepackage{hyperref}

\usepackage{listings}
\usepackage{tabularx}
\usepackage{color}
\usepackage{colortbl}
\usepackage{graphicx,import}
\usepackage{siunitx}
\usepackage[numbers]{natbib}
\usepackage{multirow}

\renewcommand{\thesection}{Projekt \arabic{section}}
\renewcommand{\thesubsection}{Aufgabe \arabic{section}.\arabic{subsection}}
\renewcommand{\thesubsubsection}{\arabic{section}.\arabic{subsection}.\alph{subsubsection})}

\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}

\begin{document}

\mainmatter
\title{Praktikum Parallele Numerik}
\titlerunning{Arbeit}
\author{Fabian Miltenberger, Sébastien Thill, Thore Mehr}
\authorrunning{Parallele Numerik}
\institute{Betreuer: Markus Hoffmann, Thomas Becker}
\date{23.07.2007}
\maketitle

\begin{abstract}Im Rahmen dieses Praktikums haben wir viel gelernt.
\end{abstract}

\section{}\label{p1}
In diesem Projekt lag der Schwerpunkt auf dem Kennenlernen der Bibliothek \emph{OpenMP} sowie deren Handhabung. Weiter ging es um den \emph{IntelThreadChecker}, ein Programm zum Analysieren von Programmcode auf potentielle Fehler in der Parallelisierung. Zu guter letzt haben wir uns mit der FEM-Methode beschäftigt, dabei im Speziellen mit dem Gauß-Seidel-Verfahren zum Lösen linearer Gleichungen wie sie bei der Differenzenmethode vorkommen. Zu guter letzt betrachteten wir einige andere Verfahren zum Lösen solcher Probleme und haben das CG-Verfahren implementiert.

\subsection{OpemMP}\label{p1a1}

\subsubsection{}\label{p1a1a}
Wie in der Ausgabe \ref{helloworldoutput} zu sehen, folgt die Reihenfolge der ausgeführten Fäden keinem bestimmten Muster. Auch die Reihenfolge zwischen verschiedenen Ausführungen ist in der Regel verschieden.

\begin{lstlisting}[frame=single, captionpos=b, caption={Beispielhafte Ausgabe des Programms bei Ausführung mit 8 Fäden.}, label ={helloworldoutput}, basicstyle=\footnotesize]
Hello World, this is Thread0
Hello World, this is Thread5
Hello World, this is Thread4
Hello World, this is Thread7
Hello World, this is Thread1
Hello World, this is Thread3
Hello World, this is Thread6
Hello World, this is Thread2
\end{lstlisting}

\subsubsection{}\label{p1a1b}
Hier kommt der Text zum b-Teil.

\subsubsection{}\label{p1a1c}
Hier kommt der Text zum c-Teil.




\subsection{Testtools}\label{p1a2}

\subsubsection{}\label{p1a2a}
Hier kommt der Text zum a-Teil.

\subsubsection{}\label{p1a2b}
Hier kommt der Text zum b-Teil.

\subsubsection{}\label{p1a2c}
Hier kommt der Text zum c-Teil.



\subsection{Parallelisierung}\label{p1a3}
\subsubsection{}
	Im Folgenden einige bekannte Größen der Parallelisierung, wie sie etwa in der Vorlesung Rechnerstrukturen \cite{RechnerstrukturenVL} gelehrt wurden. \\
	
	Vorab sei $T(n)$ die Ausführungszeit auf n Prozessoren,
	$P(n)$ die Anzahl der auszuführenden Einheitsoperationen und
	$I(n)$ der Parallelindex. \\
	
	Der Speedup/die Beschleunigung $S(n)$:
	\begin{equation}
		S(n) = \frac{T(1)}{T(n)}
	\end{equation}
	
	Die Effizienz $E(n)$:
	\begin{equation}
		E(n) = \frac{S(n)}{n} = \frac{T(1)}{n \cdot T(n)}
	\end{equation}
	
	Der Mehraufwand $R(n)$:
	\begin{equation}
		R(n) = \frac{P(n)}{P(1)}
	\end{equation}
	
	Die Auslastung $U(n)$:
	\begin{equation}
		U(n) = \frac{I(n)}{n} = R(n) \cdot E(n) = \frac{P(n)}{n \cdot T(n)}
	\end{equation}


\subsubsection{}	
\begin{description}
	\item[Race Condition] \hfill \\
	Auch bekannt als Wettlaufsituation. Mehrere Fäden greifen lesend und schreibend auf die gleiche Variable zu. Durch den gleichzeitigen Zugriff auf die Variable durch die anderen Fäden hängt das Ergebnis von der konkreten Ausführungsreihenfolge ab. Das Ergebnis kann der Erwartung entsprechen, muss aber nicht. Die Lösung hierfür ist Synchronisation, beispielsweise mittels atomarer Variablenzugriffe oder Barriere. Damit kann sicher gestellt werden, dass kein anderer Faden die Variable manipuliert während ein Faden mit ihr arbeitet.
	
	\item[Dead lock] \hfill \\
	Auch bekannt als Verklemmung. Erfordert mindestens zwei Fäden, die gegenseitig auf eine Resource warten, die gerade vom jeweils anderen Faden bereits allokiert wurde (formal: Zyklus im Allokationsgraphen). Damit tatsächlich alle Beteiligten Fäden nicht mehr weiter laufen können, müssen folgende Bedingungen erfüllt sein:
	\begin{itemize}
		\item Resourcen können nicht von außen freigegeben werden
		\item Fäden können weitere Resourcen anfordern, und gleichzeitig weiterhin andere halten
		\item Eine Resource kann immer nur von einem Faden gehalten betreten werden
		\item Es besteht eine zyklische Abhängigkeit im Allokationsgraphen
	\end{itemize}

	\item[Bibliotheken] \hfill \\
	Eine Fehlerquelle kann die Verwendung von Bibliotheken in einem parallelen Kontext darstellen, die nicht hierfür konstruiert wurden. Wenn eine Bibliothek beispielsweise von verschiedenen Fäden aus aufgerufen wird, aber intern keine Synchronisationsmechanismen bereithält, kann es auch hier zu Wettlaufsituationen kommen, die jedoch unter Umständen noch schwieriger zu lokalisieren sind.
	Auch kann es passieren, dass eine geplante Parallelisierung sich aufgrund Synchronisationsmechanismen innerhalb einer Bibliothek nicht wie erwartet umsetzen lässt. In diesem Fall wird die Implementierung der Bibliothek zum Flaschenhals der Parallelisierung.
	Um solche Fehler zu vermeiden, blockieren manche Bibliotheken Aufrufe, die nicht aus einem bestimmten Faden stammen. Verlassen sollte sich der Entwickler auf solche Mechanismen jedoch nicht. Viele Bibliotheken geben inzwischen in ihrer Dokumentation Hinweise auf den möglichen Einsatz im parallelen Kontext.
	
	\item[Messeffekt] \hfill \\
	Wenn man das Debuggen zum Softwareentwurf zählt, so wären als weitere Fehlerquelle auch \emph{Messeffekte} zu nennen. Verbreitete Methoden zum Debuggen, wie etwa die Ausgabe von Information auf der Konsole, können bereits selbst die Ausführung des parallelen Programms beeinflussen. So kann es passieren, dass die Mechanismen, die einen Fehler aufspüren sollen, dazu führen, dass dieser nicht weiter auftritt.
	
	\item[Cacheeffekte] \hfill \\
	Spielt ebenfalls weniger im Entwurf eine Rolle, als in der Implementierung. Die meisten Prozessoren haben einen eigenen Cache für jeden Rechenkern. Wenn nun verschiedene Fäden auf verschiedenen Kernen gleichzeitig auf der gleichen Variablen arbeiten, so kann es passieren, dass mangels Synchronisation zwischen den Caches der eine Faden die vom anderen Faden verursachte Änderung nicht \emph{sieht}. Lösung hierfür sind spezielle Mechanismen des Prozessors, um Caches bei Bedarf zu Synchronisieren. In C kann man hierfür eine Variable als \texttt{volatile} definieren.
\end{description}






\subsubsection{}
Die Vor- und Nachteile verschiedener paralleler Architekturen bezüglich zweier Aspekte sind in Tabelle \ref{parallelarch} aufgetragen.
\begin{table}
	\begin{center}
		\begin{tabular}{l | p{4cm} p{4cm}}
			Architektur & Anwenderfreundlichkeit & Energieeffizienz \\
			\hline
			GPU & Gut & Mittel \\
			CPU & Gut & Gering \\
			FPGA & Gering & Sehr gut \\
			MIC & Gut & Gut
		\end{tabular}
	\end{center}
	\caption{Verschiedene Beschleuniger im Vergleich}
	\label{parallelarch}
\end{table}




\subsection{Parallelisierung}\label{p1a4}
\subsubsection{}\label{p1a4a}
	Zu aller erst nehmen wir eine Differenzierung der in der Aufgabenstellung genannten Variablen vor. Unser $n$ bezeichne dasjenige, welches in der Beschreibung des Gauß-Seidel-Verfahrens auftritt. Das in dem gegebenen Problem genannte $n$ benennen wir zu $d$ um. Die restlichen Variablen behalten ihren Namen bei. Sei ein $l$ für das Verfahren vorgegeben, dann ergeben sich einige andere Größen wie folgt:
	\begin{equation}
		\begin{split}
			d&=2^l-1 \\
			h&=\frac{1}{2^l} \\
			n&=d^2
		\end{split}
	\end{equation}
	Anschaulich sollen die Werte von $u_{x,y}$ auf einem $(d+2)\cdot(d+2)$ großen Gitter berechnet werden. Die Abstände zwischen den Gitterpunkten sei dabei Gitterkonstante $h$. Durch die Vorgabe $u_{x,y}=0$, für $x,y$ auf dem Rand, vereinfacht sich die Problemstellung auf ein Gitter der Größe $d\cdot d$, da der Rand implizit als $0$ angenommen werden kann. Zum Lösen des Problems geben wir den Gitterpunkten $u_{x,y}$ eine Ordnung, sodass $u$ sich als einfacher Spaltenvektor mit $n=d\cdot d$ Elementen auffassen lässt. Anschaulich sieht diese Ordnung wie folgt aus:
	\begin{equation}
	u = (u_{1,1} \dots u_{d,1} u_{1,2} \dots u_{d,d})^T
	\end{equation}
	Der Definitionsbereich von $u$ (als Funktion aufgefasst) sei $\mathbb{R}^2$ (ergibt sich aus den gegebenen Randbedingungen). Damit befindet sich ein Gitterpunkt $u_{x,y}$ an der Position $(x\cdot h, y \cdot h)$. Damit berechnen wir den Wert von $f$ für alle unsere Gitterpunkte, also $f_{x,y}=f(x\cdot h,y\cdot h)$. Für diese berechneten Werte von $f$ nehmen wir die gleiche Ordnung vor wie für $u$ und können somit $f$ ebenfalls als einen Spaltenvektor der Größe $n$ ansehen.
	
	Die Matrix $A\in\mathbb{R}^{n\times n}$ sei definiert wie vorgegeben und ihre Elemente seien $(a_{i,j})$. Wir werden sehen, dass wir zur Lösung $A$ nicht explizit berechnen und speichern müssen. Nun sind alle Zutaten zur Berechnung von $u$ in $Au=h^2f$ mittels Gauß-Seidel-Verfahren gegeben.
	
	Die Implementierung geht nun Schritt für Schritt vor wie im Algorithmus vorgegeben. $u^k$ sei $u$ in der $k$-ten Iterierten von $u$. Wir beginnen mit $u^0=0$, wählen also 0 als Startvektor. Dies geschieht der Einfachheit wegen, es hat sich herausgestellt, dass es hierdurch bei keiner der gestellten Aufgaben zu Problemen kommt.
	
	In jeder Iterierten $k$ soll nun $u^{k+1}$ berechnet werden. dies geschieht nach der gegebenen Berechnungsvorschrift
	\begin{equation}
		u_j^{k+1}:=\frac{1}{a_{j,j}}(h^2f_j-\sum_{i=1}^{j-1}a_{j,1}u_i^{k+1}-\sum_{i=j+1}^{n}a_{j,i}u_i^k)
	\end{equation}
	für $j=1,\dots,n$. Es fällt auf, dass für jedes $u_j$ nur solche $u_i$ betrachtet werden, die bereits in der gleichen oder vorherigen Iterierten berechnet, und noch nicht überschrieben wurden. Somit müssen die einzelnen Iterierten $u^k$ nicht explizit gespeichert werden, tatsächlich reicht hierfür ein Vektor $u$ aus. Die Berechnungsvorschrift vereinfacht sich zu
	\begin{equation}\label{gsv_unifiedu}
		u_j=\frac{1}{a_{j,j}}(h^2f_j-\sum_{i=1, i\neq j}^{n}a_{j,1}u_i)
	\end{equation}
	Nach der Definition von $A$ ist klar, dass $a_{j,j}=4$ gilt. Für $i\neq j$ gilt $a_{i,j}\in\{0,-1\}$. Um die dünne Struktur von $A$ weiter auszunutzen, betrachten wir anschaulich, von welchen Gitterpunkten die Berechnung für einen Gitterpunkt $(x,y)$ konkret abhängt (genau das wird von $A$ beschrieben). Dargestellt wird dies in Abbildung\ref{gsv_depend}, wobei $A$ genau für die vier Nachbarknoten den Wert $-1$ annimmt, sonst 0 (außer für den Knoten selbst). Durch diese Betrachtung zerfällt die Summe der Zuweisung \ref{gsv_unifiedu} in vier bedingte Additionen, wie in Code \ref{conditionalsum} illustriert.
	\begin{lstlisting}[frame=single, captionpos=b, caption={Ausnutzung der dünnen Struktur von $A$ zur Berechnung von $u_j$. Die Zeilen in $u$ sind genau $d$ Elemente lang, daher entspricht bspw. $j - d$ dem Zugriff auf den oberen Nachbar von $j$ aus gesehen. Die Bedingungen Prüfen genau darauf, ob es sich bei einem Nachbarknoten um einen Randpunkt handelt. Falls ja, so ist keine weitere Betrachtung nötig, da dessen Wert $0$ ist.}, label={conditionalsum}, basicstyle=\small, language=C]
double sum = h * h * f[j];
if (j % d > 0) sum += u[j - 1];     // Linker Nachbarknoten
if (j % d < d - 1) sum += u[j + 1]; // Rechter Nachbarknoten
if (j / d > 0) sum += u[j - d];     // Oberer Nachbarknoten
if (j / d < d - 1) sum += u[j + d]; // Unterer Nachbarknoten
u[j] = sum / 4;
	\end{lstlisting}
	
	
	\begin{figure}
		\centering
		\def\svgwidth{0.6\textwidth}
		\input{gsv_node_dependence.pdf_tex}
		\caption{Abhängigkeiten zu Nachbarknoten, um Knoten $u^{k+1}_{x,y}$ zu berechnen. Es fällt auf, dass zwei Werte der gleichen Iterierten $k+1$ benötigt werden (die roten Knoten). Durch diese Abhängigkeit können nicht alle Einträge einer Iterierten gleichzeitig, das heißt parallel, berechnet werden.}
		\label{gsv_depend}
	\end{figure}
	
	Es wurde gezeigt, wie eine einzelne Iterierte $k$ von $u$ sich berechnen lässt. Würde man dies nun immer wiederholen, so würde sich die Lösung $u$ einem Optimum annähern. Nun haben wir aber eine begrenzte Rechenzeit und müssen daher die Berechnung irgendwann abbrechen, sobald uns das Ergebnis \emph{gut genug} ist. In der Regel kennen wir jedoch die analytische Lösung nicht und wissen daher nicht, wie gut unsere bisherige Lösung ist (sie kann ohnehin durch das Verfahren selbst stark davon abweichen, wie wir in Aufgabe \ref{p1a5} sehen werden).
	Um das Problem zu lösen, brechen wir ab, sobald der \emph{Fortschritt} zwischen zwei Iterationen klein genug ist. Die Annahme hierbei ist, dass sich die Lösung auch durch weitere Iterationen nur noch wenig ändert.
	Konkret betrachten wir die maximale Veränderung eines Eintrags zwischen den Iterierten $k$ und $k+1$ von $u$. Formal $maxError:=\|u^{k+1}-u^k\|_{max}$. Unsere Lösung nehmen wir als gut genug an, sobald $maxError<\epsilon_{Error}$ für eine Schranke $\epsilon_{Error}$.
	Für unsere Implementierung verwenden wir $\epsilon_{Error}=\num{1e-6}$, da es einerseits ausreichend klein ist, um in unseren Tests gute Ergebnisse zu liefern, andererseits groß genug, um in absehbarer Zeit berechnet werden zu können.

\subsubsection{}\label{p1a4b}
Eine naive Parallelisierung des in Aufgabe \ref{p1a4a} implementierten Gauß-Seidel-Verfahrens ist nicht möglich, da innerhalb der Berechnung von $u^{k+1}$ -- eine naive Parallelisierung würde versuchen genau diese Berechnung zu parallelisieren, also eine einzelne Iterierte -- Abhängigkeiten zwischen den Einträgen bestehen. Für die Berechnung von $u^{i+k}_{x,y}$ werden diese Abhängigkeiten in Abbildung \ref{gsv_depend} dargestellt. Um etwa $u^{i+k}_{x,y}$ zu berechnen, werden zu erst die Werte von $u^{i+k}_{x-1,y}$ und $u^{i+k}_{x,y-1}$ aus der gleichen Iteration benötigt. Diese wiederum benötigen in gleicher Weise aktuelle Werte von Nachbarknoten.

Eine Mögliche, nicht mehr ganz so naive Lösung, bestünde darin, nun über die Diagonalen zu parallelisieren. Innerhalb der Diagonalen (in $(1,-1)-Richtung$) bestehen keine Abhängigkeiten zwischen den Knoten. Dennoch hat sich auch diese Herangehensweise nicht als Vorteilhaft erwiesen. Für jede Iterierte müssten $d$ Thread-Pools mit maximal $2d-1$ Fäden gestartet und synchronisiert werden. Der Synchronisierungsaufwand hierbei scheint um Größenordnungen größer, als die eigentliche Berechnung (in der Vorlesung \emph{Software Engineering für modernene parallele Plattformen} \cite{SoftwareEngineeringPP} wird diese Herangehensweise auch als \emph{Wavefront} bezeichnet).

\subsubsection{}\label{p1a4c}
In Aufgabe \ref{p1a4b} haben wir erläutert, dass eine naive Herangehensweise für die Parallelisierung nicht zielführend ist. Insbesondere haben wir hierfür die zahlreichen Abhängigkeiten innerhalb der Berechnung einer Iterierten verantwortlich gemacht. Um also das Gauß-Seidel-Verfahren nun doch effizient zu parallelisieren, ist eine übergeordnete Betrachtung des Problems von Nöten.

Die Idee hinter unserer Lösung besteht darin, in einem parallelisierbaren Schritt Einträge von $u$ für verschiedene Iterierte $k$ der sequentiellen Variante zu berechnen. Wie in Abbildung \ref{gsv_depend} zu sehen, hängt die Berechnung eines Knotens von den \emph{aktuellen} Werten des linken und des oberen Nachbarn ab. Diese müssen also schon vorher berechnet worden sein. Hieraus ergibt sich eine Reihenfolge, in der die Knoten berechnet werden müssen, nämlich von links oben nach rechts unten (in unserer Lösung zu \ref{p1a4b} haben wir die dabei auftretenden, parallelisierbaren Diagonalen bereits erwähnt). Diese Reihenfolge können wir beibehalten, wenn wir für jede Diagonale eine andere Iterierte berechnen.

Angenommen wir berechnen links oben $u^{k+1}_{1,1}$, dann können wir parallel dazu auch $u^{k}_{3,1}$, $u^{k}_{2,2}$ und $u^{k}_{1,3}$ berechnen (sowie auch alle weiteren Diagonalen mit je 2 Abstand). Im nächsten Schritt können wir dann alle nun noch nicht abgedeckten Diagonalen gleichzeitig berechnen. In Abbildung \ref{gsv_parallel} sind all diese Diagonalen noch einmal aufgetragen. Die Felder, über die gleichzeitig berechnet werden kann, bilden ein Schachbrettmuster.

Der Grund, weshalb immer eine Diagonale ausgelassen werden muss, liegt in den vorhandenen Datenabhängigkeiten (dieses Mal in die andere Richtung, also zu den Nachbarn rechts und unten). Angenommen, man möchte $u^{k+1}_{1,1}$ und $u^{k}_{2,1}$ parallel berechnen, dann würde man bereits für ersteren den Wert von letzterem benötigen (gemäß Abbildung \ref{gsv_depend}). Lässt man jedoch eine Diagonale frei, beispielhaft zwischen $u^{k+1}_{1,1}$ und $u^{k}_{3,1}$, so konnte man $u^{k}_{2,1}$ bereits vorher berechnen. Im nächsten Schritt (das heißt nach Synchronisierung) kann aus den berechneten $u^{k+1}_{1,1}$ und $u^{k}_{3,1}$ das dazwischen liegende $u^{k+1}_{2,1}$ berechnet werden usw.
	
	
\begin{figure}
	\centering
	\definecolor{cbblack}{RGB}{250,200,200}
	\definecolor{cbwhite}{RGB}{255,255,255}
	\renewcommand*{\arraystretch}{2}
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline
		\cellcolor{cbblack}$u^{k+1}_{1,1}$ & \cellcolor{cbwhite}$u^{k+1}_{2,1}$ & \cellcolor{cbblack}$u^{k}_{3,1}$ & \cellcolor{cbwhite}$u^{k}_{4,1}$ & \cellcolor{cbblack}$u^{k-1}_{5,1}$ & \cellcolor{cbwhite}$u^{k-1}_{6,1}$ & \cellcolor{cbblack}$u^{k-2}_{7,1}$ \\
		\hline
		\cellcolor{cbwhite}$u^{k+1}_{1,2}$ & \cellcolor{cbblack}$u^{k}_{2,2}$ & \cellcolor{cbwhite}$u^{k}_{3,2}$ & \cellcolor{cbblack}$u^{k-1}_{4,2}$ & \cellcolor{cbwhite}$u^{k-1}_{5,2}$ & \cellcolor{cbblack}$u^{k-2}_{6,2}$ & \cellcolor{cbwhite}$u^{k-2}_{7,2}$ \\
		\hline
		\cellcolor{cbblack}$u^{k}_{1,3}$ & \cellcolor{cbwhite}$u^{k}_{2,3}$ & \cellcolor{cbblack}$u^{k-1}_{3,3}$ & \cellcolor{cbwhite}$u^{k-1}_{4,3}$ & \cellcolor{cbblack}$u^{k-2}_{5,3}$ & \cellcolor{cbwhite}$u^{k-2}_{6,3}$ & \cellcolor{cbblack}$u^{k-3}_{7,3}$ \\
		\hline
		\cellcolor{cbwhite}$u^{k}_{1,4}$ & \cellcolor{cbblack}$u^{k-1}_{2,4}$ & \cellcolor{cbwhite}$u^{k-1}_{3,4}$ & \cellcolor{cbblack}$u^{k-2}_{4,4}$ & \cellcolor{cbwhite}$u^{k-2}_{5,4}$ & \cellcolor{cbblack}$u^{k-3}_{6,4}$ & \cellcolor{cbwhite}$u^{k-3}_{7,4}$ \\
		\hline
		\cellcolor{cbblack}$u^{k-1}_{1,5}$ & \cellcolor{cbwhite}$u^{k-1}_{2,5}$ & \cellcolor{cbblack}$u^{k-2}_{3,5}$ & \cellcolor{cbwhite}$u^{k-2}_{4,5}$ & \cellcolor{cbblack}$u^{k-3}_{5,5}$ & \cellcolor{cbwhite}$u^{k-3}_{6,5}$ & \cellcolor{cbblack}$u^{k-4}_{7,5}$ \\
		\hline
		\cellcolor{cbwhite}$u^{k-1}_{1,6}$ & \cellcolor{cbblack}$u^{k-2}_{2,6}$ & \cellcolor{cbwhite}$u^{k-2}_{3,6}$ & \cellcolor{cbblack}$u^{k-3}_{4,6}$ & \cellcolor{cbwhite}$u^{k-3}_{5,6}$ & \cellcolor{cbblack}$u^{k-4}_{6,6}$ & \cellcolor{cbwhite}$u^{k-4}_{7,6}$ \\
		\hline
		\cellcolor{cbblack}$u^{k-2}_{1,7}$ & \cellcolor{cbwhite}$u^{k-2}_{2,7}$ & \cellcolor{cbblack}$u^{k-3}_{3,7}$ & \cellcolor{cbwhite}$u^{k-3}_{4,7}$ & \cellcolor{cbblack}$u^{k-4}_{5,7}$ & \cellcolor{cbwhite}$u^{k-4}_{6,7}$ & \cellcolor{cbblack}$u^{k-5}_{7,7}$ \\
		\hline
	\end{tabular}
	\caption{Verdeutlichung der Vorgehensweise der Parallelisierung. Zuerst werden diejenigen Einträge von $u$ parallel berechnet, die sich in grau Markierten Feldern befinden. Anschließend parallel die Einträge in den weißen Feldern. Es ist zu beachten, dass die Einträge $u_{x,y}$ für unterschiedliche Iterierte $k$ berechnet werden.}
	\label{gsv_parallel}
\end{figure}

Durch diese Schachbrett-artige Parallelisierung hat ein Berechnungsschritt die Gestalt:
\begin{itemize}
	\item Berechne Parallel über alle schwarzen Felder
	\item Berechne Parallel über alle weißen Felder
\end{itemize}
Es sei nicht zu verschweigen, dass hier im Gegensatz zur seriellen Variante sehr wohl mehrere $u$ gleichzeitig zu speichern sind. Sobald in einem Berechnungsschritt das Feld ganz rechts unten, also $u^{k+2-d}_{d,d}$, berechnet wurde, müssen im Falle des Abbruchs (weil das Abbruchkriterium aus \ref{p1a4a} erfüllt ist), alle $u^{k+2-d}_{x,y}$ noch immer bekannt sein, um $u^{k+2-d}$ als Ergebnis ausgeben zu können. Ebenso muss das Abbruchkriterium stets auf den Werten der gleichen Iterierten arbeiten, um exakt das gleiche Resultat wie die serielle Variante zu erhalten.

In jedem Berechnungsschritt wird genaue eine Iterierte des Verfahrens berechnet. Allerdings gilt dies erst, sobald $d-1$ Schritte ausgeführt wurden, denn erst dann ist schließlich der Eintrag rechts unten $u^{k+2-d}_{d,d}=u^{1}_{d,d}$ berechnet (die \emph{Historie} muss erst gefüllt werden). So kommt es, dass die parallele Version bei gleichen Eingaben exakt die gleichen Ergebnisse wie die serielle Version ausgibt, jedoch dabei genau $d-1$ Iterationen mehr benötigt. Wie der Tabelle \ref{gsv_runtime} zu entnehmen, ist die parallele Version mit 32 Kernen bei großer Problemgröße ($l=9$) mit einem \emph{Speedup} von $7,57$ aber immerhin noch deutlich schneller. Die eher geringe Effizienz schreiben wir dem Synchronisationsaufwand sowie den eher Cache-unfreundlichen Speicherzugriffen zu.

\begin{table}
	\centering
	\begin{tabular}{r|r|r||r||r|r||r|r|@{ \dots}|r|r}
		\multicolumn{3}{c||}{Problemgröße} & Seq. & \multicolumn{2}{c||}{2 Threads}  & \multicolumn{2}{c|@{ \dots}|}{4 Threads} & \multicolumn{2}{c}{32 Threads} \\
		$l$ & $d$ & $n$ & Laufzeit & Laufzeit & Speedup & Laufzeit & Speedup & Laufzeit & Speedup \\
		\hline
		4 & 15 & 225 & 0,005 & 0,007 & 0,357 & 0,008 & 0,625 & 1,361 & 0,0389 \\
		5 & 31 & 961 & 0,053 & 0,056 & 0,473 & 0,049 & 1,08 & 4,041 & 0,178 \\
		\hline
		\hline
		8 & 255 & 65.025 & 115 & 81 & 1,42 & 45,3 & 2,54 & 25,33 & 4,54 \\
		9 & 511 & 261.121 & 1340 & 943 & 1,42 & 504 & 2,67 & 177 & 7,57 \\
	\end{tabular}
	\caption{Laufzeiten unserer Parallelisierung gegenüber der sequentiellen Version. Getestet wurde auf dem Rechner \emph{i82sn07}. Um Ungenauigkeiten auszugleichen wurden die Zeiten jeweils über $100$ (für große Eingaben über $10$) Durchläufe gemittelt.}
	\label{gsv_runtime}
\end{table}
	
	

\subsection{Partielle Differentialgleichungen}\label{p1a5}
\subsubsection{}
Die Bedingungen lauten nach den Folien der FEM-Einführung: \\
	$\Omega$ sei ein beschränktes Gebiet\\
	$\Gamma$ sei hinreichend glatt\\
	$f:\Omega\rightarrow \mathbb{R}$ gegebene Funktion, wie es hier der Fall ist.\\


\subsubsection{}
Gesucht ist $f$ mit
	\begin{equation}\label{udef}
		u(x,y)=\sin(2M\pi x)\sin(2N\pi y)
	\end{equation}
	Dies kann durch einfache Anwendung des \emph{Laplace-Operators $\Delta$} berechnet werden:
	\begin{equation}\label{fgleichung}
	\begin{split}
		f(x,y)&=-\Delta u(x,y) \\
		&= -\frac{\partial u}{\partial x^2}-\frac{\partial u}{\partial y^2} \\
		&= -\frac{\partial}{\partial x}(2M\pi\cos(2M\pi x)\sin(2N\pi y)) -\frac{\partial}{\partial y}(2N\pi\sin(2M\pi x)\cos(2N\pi y)) \\
		&= 4M^2\pi^2\sin(2M\pi x)\sin(2N\pi y) + 4N^2\pi^2\sin(2M\pi x)\sin(2N\pi y) \\
		&= (M^2+N^2)4\pi^2\sin(2M\pi x)\sin(2N\pi y)
	\end{split}
	\end{equation}


\subsubsection{}
Da in Gleichung \ref{fgleichung} $M,N \in\mathbb{N}$ beliebig, wählen wir der Einfachheit halber $M=N=1$ zur Lösung dieser Teilaufgabe. Damit ergibt sich
\begin{equation}\label{fdef}
f(x,y)=8\pi^2\sin(2\pi x)\sin(2\pi y)
\end{equation}
Die dazugehörige analytische Lösung ist dann nach Aufgabenstellung gegeben als
\begin{equation}
u(x,y)=\sin(2\pi x)\sin(2\pi y)
\end{equation}
und wird verwendet um den maximalen Fehler der Näherung zu berechnen.

Das $f$ auf Gleichung \ref{fdef} kann einfach in den Code der vorangegangenen Aufgabe eingesetzt werden. Wie auch \cite{FiniteElemente} entnommen werden kann, lässt sich das Problem mit dem in Aufgabe \ref{p1a4a} gelösten Problem lösen. Es ergeben sich die in Abbildung \ref{grapha5} gezeigten Näherungen. Mit zunehmendem $l$ bzw. abnehmender Gitterkonstante $h$ werden die Lösungen nicht nur feiner, sondern der maximale Fehler zur analytischen Lösung von $u$ auch kleiner wird. Ab $l=8$ wird der Fehler allerdings wieder größer, wie Tabelle \ref{tablea5} entnommen werden kann.
An dieser Stelle scheint das für das Abbruchkriterium gewählte $\epsilon_{Error}$ aus Aufgabe \ref{p1a4a} zu grob gewählt zu sein, denn eine weitere Verfeinerung dessen reduzierte den Fehler der Näherung (nur) für große $l$ deutlich (was die Frage aufwirft, ob man $\epsilon_{Error}$ abhängig von $l$ wählen sollte).

\begin{figure}
	\centering
	\includegraphics[scale=0.18]{a5l2}
	\includegraphics[scale=0.18]{a5l3}
	\includegraphics[scale=0.18]{a5l4}
	\caption{Näherungsweise Lösung für $u$, berechnet mittels Gauß-Seidel-Verfahren. Von links nach rechts: $l=2, l=3, l=4$. Geplottet mittels GNU~Octave.}
	\label{grapha5}
\end{figure}

\begin{table}
	\centering
	\begin{tabular}{r||r|r|r|r|r|r|r|r}
		$l$ & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
		$d$ & 3 & 7 & 15 & 31 & 63 & 127 & 255 & 511 \\
		$h$ & 0,25 & 0,125 & 0,0625 & 0,0313 & 0,0156 & 0,00781 & 0,00391 & 0,00195 \\
		\hline
		Maximaler Fehler & 0,234 & 0,053 & 0,0130 & 0,00326 & 0,000866 & 0,000326 & 0,00174 & 0.0266 \\
		Iterationen & 21 & 64 & 175 & 409 & 1148 & 3515 & 11043 & 98558
	\end{tabular}
	\caption{Maximaler Fehler und Anzahl der durchgeführten Operationen in Abhängigkeit von $l$ bzw. Gitterkonstante $h$.}
	\label{tablea5}
\end{table}

Das verwendete Vorgehen wird als \emph{h-FEM} bezeichnet, da wir lediglich die Gitterkonstante $h$ anpassen. Eine \emph{p-FEM}- oder gar \emph{hp-FEM}-Methodik würde den Grad des zum Abtasten verwendeten Polynoms erhöhen.



\subsection{Partielle Differentialgleichungen}\label{p1a6}
\subsubsection{}\label{p1a6a}
Alphabetische Liste gängiger Krylow-Unterraum-Verfahren (nach \cite{KrylowUnterraumWiki}):
\begin{itemize}
	\item Arnoldi-Verfahren, zur Eigenwertapproximation
	\item BiCG, das CG-Verfahren für nicht SPD-Matrizen
	\item BiCGSTAB, Stabilisierung von CGS
	\item BiCGSTAB(ell), Stabilisierung von CGS
	\item BiCGSTABTFQMR, der Ansatz hinter TFQMR angewandt auf BiCGSTAB
	\item BiOres, eine Variante des BiCG-Verfahrens
	\item BiOmin, eine Variante des BiCG-Verfahrens
	\item BiOdir, eine Variante des BiCG-Verfahrens
	\item CG, zur approximativen Lösung linearer Gleichungssysteme
	\item CGNE, CG-Verfahren auf den Normalgleichungen, Variante 1
	\item CGNR, CG-Verfahren auf den Normalgleichungen, Variante 2
	\item CGS-Verfahren, quadrierte BiCG-Rekursion
	\item FOM, zur approximativen Lösung linearer Gleichungssysteme
	\item GMRES, zur approximativen Lösung linearer Gleichungssysteme
	\item Hessenberg-Verfahren, zur Eigenwertapproximation
	\item Lanczos-Verfahren, zur Eigenwertapproximation
	\item MinRes, zur approximativen Lösung linearer Gleichungssysteme
	\item Orthores, Orthomin und Orthodir, Verallgemeinerungen des CG-Verfahrens
	\item Ores, eine Variante des CG-Verfahrens
	\item Omin, eine Variante des CG-Verfahrens
	\item Odir, eine Variante des CG-Verfahrens
	\item Potenzmethode, älteste Methode zur Eigenwertapproximation
	\item QMR, zur approximativen Lösung linearer Gleichungssysteme
	\item Richardson-Iteration, bei geeigneter Interpretation
	\item SymmLQ, zur approximativen Lösung linearer Gleichungssysteme
	\item TFQMR, zur approximativen Lösung linearer Gleichungssysteme
\end{itemize}


Die häufig verwendeten Grundverfahren sind: CG-Verfahren, GMRES und Lanczos-Verfahren

CG-Verfahren: Hierbei handelt es sich um einen iterativen Algorithmus und ist gedacht für große lineare, symmetrische, positiv definite und dünn besetzte Gleichungssysteme. Das CG-Verfahren liefert nach spätestens n (Dimension der Matrix) Schritten die exakte Lösung.

GMRES: Ein iteratives Verfahren welches sich besonders für große, dünn besetzte lineare Gleichungssysteme eignet. GMRES kann und wird für nicht-symmetrische Matrizen eingesetzt, allerdings wird die exakte Lösung erst nach endlich vielen Schritten geliefert.

Lanczos-Verfahren: Ebenfalls ein iterativer Algorithmus, welcher Eigenwerte mit den entsprechenden Eigenvektoren bestimmen kann, als auch lineare Gleichungssysteme lösen. Die Konvergenz vom Algorithmus ist von den Eigenwerten abhängig.

Bei der Auswahl von diesen drei Verfahren für unser Problem, eignet sich das CG-Verfahren am besten, da die vom Problem gegebene Matrix alle Kriterien erfüllt. Des Weiteren ist die Konvergenz $n$ (nach \cite{Numa}) von Vorteil, da wir davon ausgehen können, dass wir schon viel früher eine Lösung erreicht haben werden welche unter einer gewissen Toleranz \emph{gut genug} sein wird.
Gegen GMRES spricht nur die möglicherweise langsamere Konvergenz und die hier nicht benötigte Flexibilität auch nicht-symmetrische Matrizen bearbeiten zu können.
Das Lanczos-Verfahren wäre eine Alternative, vor allem da die Eigenwerte und somit die Konvergenz im Voraus bekannt sind.

Implementierung soll mit Hilfe der \emph{kennengelernten Werkzeuge} analysiert werden.



\subsubsection{}\label{p1a6b}
Unsere parallele Version des CG-Verfahrens konnte gegenüber dem Gauß-Seidel-Verfahren das Problem aus Aufgabe \ref{p1a5} bis zu $40$-mal so schnell lösen. Eine Übersicht über den \emph{Speedup} sowie die \emph{Efficiency} ist in Tabelle \ref{a5a6speedupefficiency} zu finden.

\begin{table}
	\centering
	\begin{tabular}{r|r|r||r||r|r|@{ \dots}|r|r||r|r}
		\multicolumn{3}{c||}{Problemgröße} & Seq. & \multicolumn{2}{c|@{ \dots}|}{2 Threads} & \multicolumn{2}{c||}{16 Threads}  & \multicolumn{2}{c}{32 Threads} \\
		$l$ & $d$ & $n$ & Speedup & Speedup & Efficiency & Speedup & Efficiency & Speedup & Efficiency \\
		\hline
		4 & 15 & 225 & 1,74 & 1,81 & 0,906 & 2,44 & 0,153 & 2,93 & 0,0917 \\
		5 & 31 & 961 & 4,01 & 3,71 & 1,86 & 1,44 & 0,0898 & 3,46 & 0,109 \\
		\hline
		\hline
		8 & 255 & 65.025 & 17,9 & 16,5 & 8,27 & 16,8 & 1,05 & 4,05 & 0,127 \\
		9 & 511 & 261.121 & 27,6 & 25,9 & 12,9 & 39,5 & 2,47 & 37,9 & 1,18
	\end{tabular}
	\caption{\emph{Speedup} und \emph{Efficiency} für das CG-Verfahren gegenüber dem Gauß-Seidel-Verfahren für das Problem aus Aufgabe \ref{p1a5}. Die zugrunde liegenden Laufzeiten wurden anhand des Rechners \emph{i82sn07} ermittelt.}
	\label{a5a6speedupefficiency}
\end{table}

Wie auch unsere Implementierung des Gauß-Seidel-Verfahrens aus Aufgabe \ref{p1a4}, nutzen wir hier für unsere Implementierung die dünne Struktur der Matrix $A$ aus. Eine Vorkonditionierung würde diese günstige Struktur beeinträchtigen, wodurch ein sehr viel höherer Rechenaufwand zu erwarten wäre. Gesetzt den Fall unsere Matrix $A$ wäre nicht dünn besetzt, so könnte eine Vorkonditionierung Sinn machen, um die Stabilität des Algorithmus ggf. zu erhöhen.



\subsubsection{}\label{p1a6c}
In Tabelle \ref{MathBibs} haben wir ein drei Bibliotheken zur Anwendung der Finiten Elemente Methode aufgelistet und verglichen. Weitere Bibliotheken, die es nicht in die Tabelle geschafft haben, wären beispielsweise Feel++\cite{Feel++} und
Libmesh\cite{Libmesh}.

Im Rahmen dieses Praktikums würde sich, zumindest für Projekt 1, \emph{HiFlow3} am gut eignen, da hier eine ausgereifte OpenMP Funktionalität gegeben zu sein scheint, die den Portierungsaufwand stark verringert.

\begin{table}
	\centering
	\begin{tabular}{l|p{0.6\textwidth}|l}
		Bibliothek & Unterschiede & Gemeinsamkeiten \\
		\hline
		HiFlow3\cite{HiFlow3} &
		MPI und OpenMP basiert, \\
		&Hohe Parallelität (von Laptop bis zu Cluster), \\
		&Keine externen Bibliotheken benötigt (optional Libraries, unter anderem OpenMP)
		& \multirow{9}{0.3\textwidth}{Für C++ gedacht, \newline
			Implementieren Krylov-Unterraumverfahren,
			Diskretisierung wählbar} \\
		
		\cline{1-2}
		MFEM\cite{MFEM} &
		MPI basiert (und experimenteller OpenMP Support), \\
		&Sehr hohe Parallelität (mehrere Hundert Tausend Kerne), \\
		&Keine externen Bibliotheken benötigt \\
		
		\cline{1-2}
		Deal.II\cite{Deal.II} &
		MPI basiert, \\
		&Hohe Parallelität (16000 Kerne mindestens), \\
		&Keine externen Bibliotheken benötigt (optional Libraries)
	\end{tabular}
	\caption{Ausgewählte Bibliotheken zur Anwendung der Finiten Elemente Methode im Vergleich.}
	\label{MathBibs}
\end{table}


\section{}

% Normaler LNCS Zitierstil
%\bibliographystyle{splncs}
%\bibliographystyle{itmalpha}
\bibliographystyle{natdin}
% TODO: Ändern der folgenden Zeile, damit die .bib-Datei gefunden wird
\bibliography{literatur}

\end{document}

