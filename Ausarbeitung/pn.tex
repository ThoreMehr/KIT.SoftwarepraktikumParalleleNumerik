\documentclass[runningheads]{llncs}

%---- Sonderzeichen-------%
\usepackage {ngerman}
%---- Codierung----%
\usepackage[latin1]{inputenc}	% for Unix and Windows
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{url}
\usepackage{llncsdoc}
%----- Mathematischer Zeichenvorrat---%
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
% fuer die aktuelle Zeit
\usepackage{scrtime}
\usepackage{listings}
\usepackage{subfigure}
\usepackage{hyperref}

\usepackage{listings}
\usepackage{tabularx}
\usepackage{color}
\usepackage{colortbl}
\usepackage{graphicx,import}
\usepackage{siunitx}
\usepackage[numbers]{natbib}
\usepackage{multirow}

\renewcommand{\thesection}{Projekt \arabic{section}}
\renewcommand{\thesubsection}{Aufgabe \arabic{section}.\arabic{subsection}}
\renewcommand{\thesubsubsection}{\arabic{section}.\arabic{subsection}.\alph{subsubsection})}
\newcommand{\TODO}[1]{\textbf{\Large\color{red}#1}}

\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}

\begin{document}

\mainmatter
\title{Softwarepraktikum Parallele Numerik}
\titlerunning{Parallele Numerik}
\author{Fabian Immanuel Miltenberger, Sébastien Thill, Thore Mehr}
\authorrunning{F. Miltenberger, S. Thill, T. Mehr}
\institute{Betreuer: Markus Hoffmann, Thomas Becker\\Lehrstuhl für Rechnerarchitektur und Parallelverarbeitung}
\date{23.07.2007}
\maketitle

\begin{abstract}Im Rahmen dieses Praktikums haben wir viel gelernt.
\end{abstract}

\section{}\label{p1}
In diesem Projekt lag der Schwerpunkt auf dem Kennenlernen der Bibliothek \emph{OpenMP} sowie deren Handhabung. Weiter ging es um den \emph{IntelThreadChecker}, ein Programm zum Analysieren von Programmcode auf potentielle Fehler in der Parallelisierung. Zu guter letzt haben wir uns mit der FEM-Methode beschäftigt, dabei im Speziellen mit dem Gauß-Seidel-Verfahren zum Lösen linearer Gleichungen wie sie bei der Differenzenmethode vorkommen. Zu guter letzt betrachteten wir einige andere Verfahren zum Lösen solcher Probleme und haben das CG-Verfahren implementiert.

\subsection{OpemMP}\label{p1a1}

\subsubsection{}\label{p1a1a}
Wie in der Ausgabe \ref{helloworldoutput} zu sehen, folgt die Reihenfolge der ausgeführten Fäden keinem bestimmten Muster. Auch die Reihenfolge zwischen verschiedenen Ausführungen ist in der Regel verschieden.

\begin{lstlisting}[frame=single, captionpos=b, caption={Beispielhafte Ausgabe des Programms bei Ausführung mit 8 Fäden.}, label ={helloworldoutput}, basicstyle=\footnotesize]
Hello World, this is Thread0
Hello World, this is Thread5
Hello World, this is Thread4
Hello World, this is Thread7
Hello World, this is Thread1
Hello World, this is Thread3
Hello World, this is Thread6
Hello World, this is Thread2
\end{lstlisting}

\subsubsection{}\label{p1a1b}
\begin{table}
	\centering
	\scriptsize
	\begin{tabular}{r||r||r|r||r|r||r|r||r|r}
		\multicolumn{1}{c||}{Problemgröße} & Seq. & \multicolumn{2}{c||}{2 Threads}  & \multicolumn{2}{c||}{16 Threads} & \multicolumn{2}{c||}{32 Threads} &
		\multicolumn{2}{c}{64 Threads} \\
		$N$ & Laufzeit & Laufzeit & Speedup & Laufzeit & Speedup & Laufzeit & Speedup & Laufzeit & Speedup \\
		\hline
		\hline
		$10^7$ critical & 0,49 & 1,02 & 0,48 & 39,33 & 0,012 & 65,18& 0,008 & 93,21& 0,005\\
		\hline
		\hline
		$10^7$ reduction & 0,29 & 0,15 & 1,99 & 0,044 & 6,59 & 0,041& 7,07 & 0,05& 5,8\\
		\hline
		$10^8$ reduction & 2,96 & 1,48 & 2 & 0,215 & 13,78 & 0,157& 18,85 & 0,185& 16\\
		\hline
		$10^9$ reduction & 28,76 & 14,77 & 1,95 & 1,86 & 15,46 &0,99 &29,05& 1,14 & 25,25\\
	\end{tabular}
	\caption{Laufzeiten der Berechnung von Pi unserer Parallelisierung gegenüber der sequentiellen Version. Getestet wurde auf dem Rechner \emph{i82sn07}. Um Ungenauigkeiten auszugleichen wurden die Zeiten jeweils über $10$ (für große Eingaben über $5$) Durchläufe gemittelt.}
	\label{pi_runtime}
\end{table}
Wie in Tabelle \ref{pi_runtime} zu sehen, nähert sich der \emph{Speedup} mit zunehmenden $N$ der Anzahl der Kerne an.\\
Die manuelle Variante hat eine schlechtere Performance, als die Variante ohne Parallelisierung. Dies lässt sich damit begründen, dass \emph{OpenMP} bei der Variante mit \emph{Reduction} weitere Optimierungen umsetzen kann, beispielsweise kann jeder Thread erst einmal all seine eigenen Beiträge aufsummieren, bevor am Ende nach einer einzigen Synchronisation (anstatt bei jeder Schleifenausführung) die Ergebnisse aller Threads aufsummiert werden.
\subsubsection{}\label{p1a1c}
\begin{table}
	\centering
	\begin{tabular}{r||r||r|r||r|r||r|r}
		\multicolumn{1}{c||}{Problemgröße} & Seq. & \multicolumn{2}{c||}{2 Threads}  & \multicolumn{2}{c||}{16 Threads} & \multicolumn{2}{c}{32 Threads} \\
		 $N$ & Laufzeit & Laufzeit & Speedup & Laufzeit & Speedup & Laufzeit & Speedup \\
		\hline
		\hline
		1000 & 6,02 & 4,35 & 1,38 & 1,51 & 3,99 & 0,77 & 7,88 \\
		\hline
		2000 & 24,65 & 17,41 & 1,44 & 5,92 & 4,16 & 3,11 & 7,92 \\
		\hline
		
		4000 & 96,24 & 70,47 & 1,37 & 23,66 & 4,07 & 11,913 & 8,07  \\
		\hline
		8000 & 394,05 & 278,34 & 1,44 & 94,60 & 4,16 & 47,85 & 8,24  \\
		\hline
	\end{tabular}
	\caption{Laufzeiten des Mandelbrot Programms unserer Parallelisierung gegenüber der sequentiellen Version. Getestet wurde auf dem Rechner \emph{i82sn07}. Um Ungenauigkeiten auszugleichen wurden die Zeiten jeweils über $10$ (für große Eingaben über $5$) Durchläufe gemittelt.}
	\label{mandel_runtime}
\end{table}
Wie an der Tabelle \ref{mandel_runtime} abgelesen werden kann, beträgt die Beschleunigung für 2 Threads etwa $1,4$, für 16 Threads etwa 4 und für 32 Threads etwa 8. Dass die Beschleunigung schlechter ist, als bei der Berechnung von PI, liegt meiner Meinung nach daran, dass Speicherverwaltung und Caches einen wesentlichen Anteil der Laufzeit ausmachen.

\subsection{Testtools}\label{p1a2}
\subsubsection{}\label{p1a2a}
\begin{enumerate}
	\item {Memory read at \textbf{''demo\_with\_bugs.c'':27} conflicts with a prior memory write at \textbf{''demo\_with\_bugs.c'':26} (flow dependence)\\
		Kann behoben werden, indem die Schleife aufgeteilt wird in zwei Schleifen mit \texttt{\#pragma omp prallel for simd}.
		}
	\item {Memory read at \textbf{''demo\_with\_bugs.c'':50} conflicts with a prior memory write at \textbf{''demo\_with\_bugs.c'':45} (flow dependence)\\
		Kann behoben werden, indem das \texttt{nowait} statement weggelassen wird.
		} 
	\item {Memory read at \textbf{''demo\_with\_bugs.c'':66} conflicts with a prior memory write at \textbf{''demo\_with\_bugs.c'':66} (flow dependence)\\
		Memory write at \textbf{''demo\_with\_bugs.c'':67} conflicts with a prior memory write at \textbf{''demo\_with\_bugs.c'':66} (anti dependence)\\
		Kann behoben werden, indem man das \texttt{x} als privat markiert, etwa durch Ersetzen des Pragma durch \texttt{\#pragma omp parallel for private(x)}.
		}
	\item{ Vom Thread Checker nicht erkannt.\\ Kann behoben werden in dem das Pragma auf \texttt{\#pragma omp parallel for lastprivate(x)} geändert wird.}
	\item {Memory read at \textbf{''demo\_with\_bugs.c'':98} conflicts with a prior memory write at \textbf{''demo\_with\_bugs.c'':98} (flow dependence)\\
		Memory write at \textbf{''demo\_with\_bugs.c'':98} conflicts with a prior memory write at \textbf{''demo\_with\_bugs.c'':98} (OUTPUT dependence)\\
		 Kann behoben werden in dem das Pragma auf \texttt{\#pragma omp parallel for reduction(+:sum)} geändert wird.}
	
	
\end{enumerate}

\subsubsection{}\label{p1a2b}
Um die Ausnutzung des Caches zu gewährleisten, wird die eine Matrix als Zeilen-, die andere als Spaltenmatrix verarbeitet und nach dem ikj-Algorithmus multipliziert. Es wurden eine sequentielle, sowie drei parallelisierte Versionen untersucht. Die parallelisierten Versionen unterscheiden sich wie folgt:
\begin{enumerate}
	\item Innerste Schleife mittels \emph{Reduction} parallelisiert
	\item Mittlere Schleife parallelisiert
	\item Äußere Schleife parallelisiert
\end{enumerate}
In den Tabellen \ref{2b_1000} und \ref{2b_2000} wird der Einfluss von Optimierungsoptionen vom GCC und ICC untersucht. Die Messungen wurden auf \emph{i82pc31} ausgeführt, da dieser über eine Intel CPU verfügt. Wie aus den Tabellen abgelesen werden kann, sind die Unterschiede bei den Optimierungsstufen O0 bis O3 gering. GCC scheint etwas schnelleren sequentiellen, ICC hingegen schnelleren parallelen Code zu generieren. Wenn Vektorisierung benutzt werden kann -- beim ICC mit der Option fast -- wird der Code viel schneller, allerdings auf Kosten der Größe, das Binary hat ~1,4 MB statt ~30 kB. Im Allgemeinen ist das Binary, das vom ICC erzeugt wird, etwa doppelt so groß, wie das, dass vom GCC erzeugt wird.
\begin{table}
	\centering
	\begin{tabular}{|r||r|r|r|r|r||r|r|r|r|}
		\hline
		          &     \multicolumn{5}{c||}{ICC}     &  \multicolumn{4}{c|}{GCC}  \\ \hline
		 Optionen &   O0 &   O1 &   O2 &    O3 & fast &   O0 &   O1 &   O2 &    O3 \\ \hline
      sequentiell & 9,26 & 1,19 & 1,17 &  1,17 & 0,25 & 9,56 & 1,16 & 1,16 &  1,16 \\ \hline
		reduction & 4,99 & 1,90 & 1,88 & 1,88 & 1,40 & 6,06 & 2,21 & 2,21 & 2,21 \\ \hline
		 mittlere & 5,88 & 1,02 & 1,02 &  1,02 & 0,96 & 6,95 & 2,55 & 1,12 &  1,18 \\ \hline
		   äußere & 5,67 & 0,59 & 0,59 &  0,59 & 0,014 & 6,02 & 1,81 & 0,82 &  0,82 \\ \hline
	\end{tabular}
	\caption{Laufzeiten in s mit N=1000. Getestet wurde auf dem Rechner \emph{i82pc31}. Um Ungenauigkeiten auszugleichen, wurden die Zeiten jeweils über $10$ Durchläufe gemittelt.}
	\label{2b_1000}
\end{table}

\begin{table}
	\centering
	\begin{tabular}{|r||r|r|r|r|r||r|r|r|r|}
		\hline
		&\multicolumn{5}{c||}{ICC}&\multicolumn{4}{c|}{GCC}\\
		\hline
		Optionen& O0&O1&O2&O3&fast&O0&O1&O2&O3\\		
		\hline
		sequentiell &74,58&9,9&9,88&9,86&2,27&76,75&9,76&9,75&9,73\\
		\hline
		reduction&36,67&10,99&10,98&10,95&8,54&43,20&13,76&12,54&12,48\\
		\hline
		mittlere&47,04&9,01&8,66&8,37&7,95&54,19&30,05&9,60&9,50\\
		\hline
		äußere&46,50&5,19&4,95&4,92&1,15&44,83&15,07&6,93&6,84\\
		\hline
	\end{tabular}
	\caption{Laufzeiten in s mit N=2000. Getestet wurde auf dem Rechner \emph{i82pc31}. Um Ungenauigkeiten auszugleichen, wurden die Zeiten jeweils über $10$ Durchläufe gemittelt.}
	\label{2b_2000}
\end{table}
\subsubsection{}\label{p1a2c}
Anhand der Ausgabe von \emph{OpenMP} kann leicht gesehen werden, dass wenige der Threads einen Großteil der Arbeit leisten, was bei der Verwendung von reduction zu erwarten ist. Durch die explizite Angabe des Scheduling und der Chunksize kann die Arbeit besser verteilt werden. In Tabelle \ref{2c_mandelbrot} kann der Einfluss der Optionen auf die Laufzeit abgelesen werden.
\begin{table}
\begin{tabular}{|c|c|c|c|c|c|}
	\hline
	        & auto & Chunks.=1 & Chunks.=10 & Chunks.=100 & Chunks.=1000 \\ \hline
	N=4000  & \multicolumn{4}{|l}{Maxiter=500}            &  \\ \hline
	static  & 0,99 &   0,38    &    0,35    &    0,89     &     5,02     \\ \hline
	dynamic & 0,37 &   0,31    &    0,30    &    0,84     &     5,02     \\ \hline
	N=8000  & \multicolumn{4}{|l}{Maxiter=500}            &  \\ \hline
	static  & 3,91 &   1,16    &    1,15    &    1,79     &    13,74     \\ \hline
	dynamic & 1,20 &   1,08    &    1,08    &    1,76     &    13,74     \\ \hline
	\hline
	& auto & Chunks.=1 & Chunks.=10 & Chunks.=100 & Chunks.=1000 \\ \hline
	N=4000  & \multicolumn{4}{|l}{Maxiter=1000}            &  \\ \hline
	static  & 1,91 &   0,60    &    0,54    &    1,69    &     9,70     \\ \hline
	dynamic & 0,54 &   0,51    &    0,52    &    1,64     &     9,70     \\ \hline
	N=8000  & \multicolumn{4}{|l}{Maxiter=1000}            &  \\ \hline
	static  & 1,97 &   1,95    &    2,07    &    3,44     &    26,84     \\ \hline
	dynamic & 1,99 &   1,99    &    2,00    &    3,52     &    26,85     \\ \hline
\end{tabular} 
\caption{Einfluss von Scheduling und Chunksize auf die Laufzeit von Mandelbrot. Getestet wurde auf dem Rechner \emph{i82sn07} mit 32 Threads. Um Ungenauigkeiten auszugleichen wurden die Zeiten jeweils über $10$ Durchläufe gemittelt.}
\label{2c_mandelbrot}
\end{table}



\subsection{Parallelisierung}\label{p1a3}
\subsubsection{}
	Im Folgenden einige bekannte Größen der Parallelisierung, wie sie etwa in der Vorlesung Rechnerstrukturen \cite{RechnerstrukturenVL} gelehrt wurden. \\
	
	Vorab sei $T(n)$ die Ausführungszeit auf n Prozessoren,
	$P(n)$ die Anzahl der auszuführenden Einheitsoperationen \\
	
	Der Speedup/die Beschleunigung $S(n)$:
	\begin{equation}
		S(n) = \frac{T(1)}{T(n)}
	\end{equation}
	
	Die Effizienz $E(n)$:
	\begin{equation}
		E(n) = \frac{S(n)}{n} = \frac{T(1)}{n \cdot T(n)}
	\end{equation}
	
	Der Mehraufwand $R(n)$:
	\begin{equation}
		R(n) = \frac{P(n)}{P(1)}
	\end{equation}
	
	Der Parallelindex $I(n)$:
	\begin{equation}
		I(n)=\frac{P(n)}{T(n)}
	\end{equation}
	
	Die Auslastung $U(n)$:
	\begin{equation}
		U(n) = \frac{I(n)}{n} = R(n) \cdot E(n) = \frac{P(n)}{n \cdot T(n)}
	\end{equation}


\subsubsection{}	
\begin{itemize}
	\item \textbf{Race Condition} \\
	Auch bekannt als Wettlaufsituation. Mehrere Fäden greifen lesend und schreibend auf die gleiche Variable zu. Durch den gleichzeitigen Zugriff auf die Variable durch die anderen Fäden hängt das Ergebnis von der konkreten Ausführungsreihenfolge ab. Das Ergebnis kann der Erwartung entsprechen, muss aber nicht. Die Lösung hierfür ist Synchronisation, beispielsweise mittels atomarer Variablenzugriffe oder Barriere. Damit kann sicher gestellt werden, dass kein anderer Faden die Variable manipuliert während ein Faden mit ihr arbeitet.
	
	\item \textbf{Dead lock} \\
	Auch bekannt als Verklemmung. Erfordert mindestens zwei Fäden, die gegenseitig auf eine Resource warten, die gerade vom jeweils anderen Faden bereits allokiert wurde (formal: Zyklus im Allokationsgraphen). Damit tatsächlich alle Beteiligten Fäden nicht mehr weiter laufen können, müssen folgende Bedingungen erfüllt sein:
	\begin{itemize}
		\item Resourcen können nicht von außen freigegeben werden
		\item Fäden können weitere Resourcen anfordern, und gleichzeitig weiterhin andere halten
		\item Eine Resource kann immer nur von einem Faden gehalten betreten werden
		\item Es besteht eine zyklische Abhängigkeit im Allokationsgraphen
	\end{itemize}

	\item \textbf{Bibliotheken} \\
	Eine Fehlerquelle kann die Verwendung von Bibliotheken in einem parallelen Kontext darstellen, die nicht hierfür konstruiert wurden. Wenn eine Bibliothek beispielsweise von verschiedenen Fäden aus aufgerufen wird, aber intern keine Synchronisationsmechanismen bereithält, kann es auch hier zu Wettlaufsituationen kommen, die jedoch unter Umständen noch schwieriger zu lokalisieren sind.
	Auch kann es passieren, dass eine geplante Parallelisierung sich aufgrund Synchronisationsmechanismen innerhalb einer Bibliothek nicht wie erwartet umsetzen lässt. In diesem Fall wird die Implementierung der Bibliothek zum Flaschenhals der Parallelisierung.
	Um solche Fehler zu vermeiden, blockieren manche Bibliotheken Aufrufe, die nicht aus einem bestimmten Faden stammen. Verlassen sollte sich der Entwickler auf solche Mechanismen jedoch nicht. Viele Bibliotheken geben inzwischen in ihrer Dokumentation Hinweise auf den möglichen Einsatz im parallelen Kontext.
	
	\item \textbf{Messeffekt} \\
	Wenn man das Debuggen zum Softwareentwurf zählt, so wären als weitere Fehlerquelle auch \emph{Messeffekte} zu nennen. Verbreitete Methoden zum Debuggen, wie etwa die Ausgabe von Information auf der Konsole, können bereits selbst die Ausführung des parallelen Programms beeinflussen. So kann es passieren, dass die Mechanismen, die einen Fehler aufspüren sollen, dazu führen, dass dieser nicht weiter auftritt.
	
	\item \textbf{Cacheeffekte} \\
	Spielt ebenfalls weniger im Entwurf eine Rolle, als in der Implementierung. Die meisten Prozessoren haben einen eigenen Cache für jeden Rechenkern. Wenn nun verschiedene Fäden auf verschiedenen Kernen gleichzeitig auf der gleichen Variablen arbeiten, so kann es passieren, dass mangels Synchronisation zwischen den Caches der eine Faden die vom anderen Faden verursachte Änderung nicht \emph{sieht}. Lösung hierfür sind spezielle Mechanismen des Prozessors, um Caches bei Bedarf zu Synchronisieren. In C kann man hierfür eine Variable als \texttt{volatile} definieren.
\end{itemize}






\subsubsection{}
Im Folgenden werden einige parallele Architekturen genauer erläutert. Ein grober Vergleich zur CPU kann Tabelle \ref{parallelarch} entnommen werden.
\begin{itemize}
	\item \textbf{CPU} \\
	Die CPU stellt einen großen Instruktionssatz zur Verfügung und kann komplexe Sprünge im Code abarbeiten. Durch Techniken wie Sprungvorhersagen lässt sich in diesen Fällen die Performance steigern und mit Out-of-Order Architekturen erhält man einen hohen Grad an Instruktionslevel-Parallelismus. Im Allgemeinen finden sich selten mehr als 8 CPU Kerne in einem herkömmlichen Rechner. Es bietet sich die ,,gewöhnliche'' Programmierung an.

	\item \textbf{GPU} \\
	Moderne Rechner haben fast alle eine GPU verbaut, welche anders als die CPU keine komplexen Befehle abarbeiten kann, sondern durch eine sehr hohe Anzahl an einfachen Rechnenkernen massive Parallelität bereitstellen kann. Dieser Beschleuniger arbeitet auf einem Befehlsstrom und führt diesen mehrfach aus (\emph{Single Instruction, Multiple Data} -- kurz \emph{SIMD}). Um GPUs für ein eigenes Progromm zu nutzen helfen Bibliotheken wie \emph{CUDA} (nVidia spezifisch), oder \emph{OpenCL}.

	\item \textbf{FPGA} \\
	Ein FPGA Board kann beliebige Hardware-Eigenschaften bereitstellen, sofern diese vorher auf das FPGA aufgespielt wurden. Im Allgemeinen werden FPGAs zum Testen von, Chips welche sich noch in der Entwicklung befinden, verwendet. Aber das FPGA kann auch als ,,multifunktionelle'' Beschleunigungskarte eingesetzt werden (lässt sich sehr gut an Anforderungen anpassen). Um ein FPGA zu programmieren bietet sich \emph{OpenCL} an, oder eine der Bibliotheken welche die implementierte Schaltung nutzt.

	\item \textbf{MIC} (Intel \emph{Many Integrated Core}) \\
	Die Grundidee von MICs ist, die gewöhnliche x86 CPU Architektur zur Verfügung zu stellen, aber dabei die Parallelität verglichen mit normalen CPUs zu erhöhen. Dies wird durch ein Zusammenschalten von mehreren CPUs auf einem Chip erreicht, welche dann über PCIe mit dem Hostsystem kommunizieren. Es wurde bewusst die x86 Architektur gewählt, damit bekannte Parallelisierungstools wie \emph{OpenMP} oder \emph{OpenCL} genutzt werden können. 
\end{itemize}

\begin{table}
	\centering
	\begin{tabular}{l | p{4cm} p{4cm}}
		Beschleuniger & Parallelität & Anwenderfreundlichkeit \\
		\hline
		CPU & Niedrig & Gut \\
		GPU & Sehr hoch & Mittel \\
		FPGA & Hoch & Abhängig von Bibliotheken \\
		MIC & (Sehr) hoch & Gut
	\end{tabular}
	\caption{Verschiedene Beschleuniger im Vergleich, die CPU wird als Referenz verwendet.}
	\label{parallelarch}
\end{table}

\TODO{Quellen fehlen noch}




\subsection{Parallelisierung}\label{p1a4}
\subsubsection{}\label{p1a4a}
	Zu aller erst nehmen wir eine Differenzierung der in der Aufgabenstellung genannten Variablen vor. Unser $n$ bezeichne dasjenige, welches in der Beschreibung des Gauß-Seidel-Verfahrens auftritt. Das in dem gegebenen Problem genannte $n$ benennen wir zu $d$ um. Die restlichen Variablen behalten ihren Namen bei. Sei ein $l$ für das Verfahren vorgegeben, dann ergeben sich einige andere Größen wie folgt:
	\begin{equation}
		\begin{split}
			d&=2^l-1 \\
			h&=\frac{1}{2^l} \\
			n&=d^2
		\end{split}
	\end{equation}
	Anschaulich sollen die Werte von $u_{x,y}$ auf einem $(d+2)\cdot(d+2)$ großen Gitter berechnet werden. Die Abstände zwischen den Gitterpunkten sei dabei Gitterkonstante $h$. Durch die Vorgabe $u_{x,y}=0$, für $x,y$ auf dem Rand, vereinfacht sich die Problemstellung auf ein Gitter der Größe $d\cdot d$, da der Rand implizit als $0$ angenommen werden kann. Zum Lösen des Problems geben wir den Gitterpunkten $u_{x,y}$ eine Ordnung, sodass $u$ sich als einfacher Spaltenvektor mit $n=d\cdot d$ Elementen auffassen lässt. Anschaulich sieht diese Ordnung wie folgt aus:
	\begin{equation}
	u = (u_{1,1} \dots u_{d,1} u_{1,2} \dots u_{d,d})^T
	\end{equation}
	Der Definitionsbereich von $u$ (als Funktion aufgefasst) sei $\mathbb{R}^2$ (ergibt sich aus den gegebenen Randbedingungen). Damit befindet sich ein Gitterpunkt $u_{x,y}$ an der Position $(x\cdot h, y \cdot h)$. Damit berechnen wir den Wert von $f$ für alle unsere Gitterpunkte, also $f_{x,y}=f(x\cdot h,y\cdot h)$. Für diese berechneten Werte von $f$ nehmen wir die gleiche Ordnung vor wie für $u$ und können somit $f$ ebenfalls als einen Spaltenvektor der Größe $n$ ansehen.
	
	Die Matrix $A\in\mathbb{R}^{n\times n}$ sei definiert wie vorgegeben und ihre Elemente seien $(a_{i,j})$. Wir werden sehen, dass wir zur Lösung $A$ nicht explizit berechnen und speichern müssen. Nun sind alle Zutaten zur Berechnung von $u$ in $Au=h^2f$ mittels Gauß-Seidel-Verfahren gegeben.
	
	Die Implementierung geht nun Schritt für Schritt vor wie im Algorithmus vorgegeben. $u^k$ sei $u$ in der $k$-ten Iterierten von $u$. Wir beginnen mit $u^0=0$, wählen also 0 als Startvektor. Dies geschieht der Einfachheit wegen, es hat sich herausgestellt, dass es hierdurch bei keiner der gestellten Aufgaben zu Problemen kommt.
	
	In jeder Iterierten $k$ soll nun $u^{k+1}$ berechnet werden. dies geschieht nach der gegebenen Berechnungsvorschrift
	\begin{equation}
		u_j^{k+1}:=\frac{1}{a_{j,j}}(h^2f_j-\sum_{i=1}^{j-1}a_{j,1}u_i^{k+1}-\sum_{i=j+1}^{n}a_{j,i}u_i^k)
	\end{equation}
	für $j=1,\dots,n$. Es fällt auf, dass für jedes $u_j$ nur solche $u_i$ betrachtet werden, die bereits in der gleichen oder vorherigen Iterierten berechnet, und noch nicht überschrieben wurden. Somit müssen die einzelnen Iterierten $u^k$ nicht explizit gespeichert werden, tatsächlich reicht hierfür ein Vektor $u$ aus. Die Berechnungsvorschrift vereinfacht sich zu
	\begin{equation}\label{gsv_unifiedu}
		u_j=\frac{1}{a_{j,j}}(h^2f_j-\sum_{i=1, i\neq j}^{n}a_{j,1}u_i)
	\end{equation}
	Nach der Definition von $A$ ist klar, dass $a_{j,j}=4$ gilt. Für $i\neq j$ gilt $a_{i,j}\in\{0,-1\}$. Um die dünne Struktur von $A$ weiter auszunutzen, betrachten wir anschaulich, von welchen Gitterpunkten die Berechnung für einen Gitterpunkt $(x,y)$ konkret abhängt (genau das wird von $A$ beschrieben). Dargestellt wird dies in Abbildung\ref{gsv_depend}, wobei $A$ genau für die vier Nachbarknoten den Wert $-1$ annimmt, sonst 0 (außer für den Knoten selbst). Durch diese Betrachtung zerfällt die Summe der Zuweisung \ref{gsv_unifiedu} in vier bedingte Additionen, wie in Code \ref{conditionalsum} illustriert.
	\begin{lstlisting}[frame=single, captionpos=b, caption={Ausnutzung der dünnen Struktur von $A$ zur Berechnung von $u_j$. Die Zeilen in $u$ sind genau $d$ Elemente lang, daher entspricht bspw. $j - d$ dem Zugriff auf den oberen Nachbar von $j$ aus gesehen. Die Bedingungen Prüfen genau darauf, ob es sich bei einem Nachbarknoten um einen Randpunkt handelt. Falls ja, so ist keine weitere Betrachtung nötig, da dessen Wert $0$ ist.}, label={conditionalsum}, basicstyle=\small, language=C]
double sum = h * h * f[j];
if (j % d > 0) sum += u[j - 1];     // Linker Nachbarknoten
if (j % d < d - 1) sum += u[j + 1]; // Rechter Nachbarknoten
if (j / d > 0) sum += u[j - d];     // Oberer Nachbarknoten
if (j / d < d - 1) sum += u[j + d]; // Unterer Nachbarknoten
u[j] = sum / 4;
	\end{lstlisting}
	
	
	\begin{figure}
		\centering
		\def\svgwidth{0.6\textwidth}
		\input{gsv_node_dependence.pdf_tex}
		\caption{Abhängigkeiten zu Nachbarknoten, um Knoten $u^{k+1}_{x,y}$ zu berechnen. Es fällt auf, dass zwei Werte der gleichen Iterierten $k+1$ benötigt werden (die roten Knoten). Durch diese Abhängigkeit können nicht alle Einträge einer Iterierten gleichzeitig, das heißt parallel, berechnet werden.}
		\label{gsv_depend}
	\end{figure}
	
	Es wurde gezeigt, wie eine einzelne Iterierte $k$ von $u$ sich berechnen lässt. Würde man dies nun immer wiederholen, so würde sich die Lösung $u$ einem Optimum annähern. Nun haben wir aber eine begrenzte Rechenzeit und müssen daher die Berechnung irgendwann abbrechen, sobald uns das Ergebnis \emph{gut genug} ist. In der Regel kennen wir jedoch die analytische Lösung nicht und wissen daher nicht, wie gut unsere bisherige Lösung ist (sie kann ohnehin durch das Verfahren selbst stark davon abweichen, wie wir in Aufgabe \ref{p1a5} sehen werden).
	Um das Problem zu lösen, brechen wir ab, sobald der \emph{Fortschritt} zwischen zwei Iterationen klein genug ist. Die Annahme hierbei ist, dass sich die Lösung auch durch weitere Iterationen nur noch wenig ändert.
	Konkret betrachten wir die maximale Veränderung eines Eintrags zwischen den Iterierten $k$ und $k+1$ von $u$. Formal $maxError:=\|u^{k+1}-u^k\|_{max}$. Unsere Lösung nehmen wir als gut genug an, sobald $maxError<\varepsilon_{Error}$ für eine Schranke $\varepsilon_{Error}$.
	Für unsere Implementierung verwenden wir $\varepsilon_{Error}=\num{1e-6}$, da es einerseits ausreichend klein ist, um in unseren Tests gute Ergebnisse zu liefern, andererseits groß genug, um in absehbarer Zeit berechnet werden zu können.

\subsubsection{}\label{p1a4b}
Eine naive Parallelisierung des in Aufgabe \ref{p1a4a} implementierten Gauß-Seidel-Verfahrens ist nicht möglich, da innerhalb der Berechnung von $u^{k+1}$ -- eine naive Parallelisierung würde versuchen genau diese Berechnung zu parallelisieren, also eine einzelne Iterierte -- Abhängigkeiten zwischen den Einträgen bestehen. Für die Berechnung von $u^{i+k}_{x,y}$ werden diese Abhängigkeiten in Abbildung \ref{gsv_depend} dargestellt. Um etwa $u^{i+k}_{x,y}$ zu berechnen, werden zu erst die Werte von $u^{i+k}_{x-1,y}$ und $u^{i+k}_{x,y-1}$ aus der gleichen Iteration benötigt. Diese wiederum benötigen in gleicher Weise aktuelle Werte von Nachbarknoten.

Eine Mögliche, nicht mehr ganz so naive Lösung, bestünde darin, nun über die Diagonalen zu parallelisieren. Innerhalb der Diagonalen (in $(1,-1)-Richtung$) bestehen keine Abhängigkeiten zwischen den Knoten. Dennoch hat sich auch diese Herangehensweise nicht als Vorteilhaft erwiesen. Für jede Iterierte müssten $d$ Thread-Pools mit maximal $2d-1$ Fäden gestartet und synchronisiert werden. Der Synchronisierungsaufwand hierbei scheint um Größenordnungen größer, als die eigentliche Berechnung (in der Vorlesung \emph{Software Engineering für modernene parallele Plattformen} \cite{SoftwareEngineeringPP} wird diese Herangehensweise auch als \emph{Wavefront} bezeichnet).

\subsubsection{}\label{p1a4c}
In Aufgabe \ref{p1a4b} haben wir erläutert, dass eine naive Herangehensweise für die Parallelisierung nicht zielführend ist. Insbesondere haben wir hierfür die zahlreichen Abhängigkeiten innerhalb der Berechnung einer Iterierten verantwortlich gemacht. Um also das Gauß-Seidel-Verfahren nun doch effizient zu parallelisieren, ist eine übergeordnete Betrachtung des Problems von Nöten.

Die Idee hinter unserer Lösung besteht darin, in einem parallelisierbaren Schritt Einträge von $u$ für verschiedene Iterierte $k$ der sequentiellen Variante zu berechnen. Wie in Abbildung \ref{gsv_depend} zu sehen, hängt die Berechnung eines Knotens von den \emph{aktuellen} Werten des linken und des oberen Nachbarn ab. Diese müssen also schon vorher berechnet worden sein. Hieraus ergibt sich eine Reihenfolge, in der die Knoten berechnet werden müssen, nämlich von links oben nach rechts unten (in unserer Lösung zu \ref{p1a4b} haben wir die dabei auftretenden, parallelisierbaren Diagonalen bereits erwähnt). Diese Reihenfolge können wir beibehalten, wenn wir für jede Diagonale eine andere Iterierte berechnen.

Angenommen wir berechnen links oben $u^{k+1}_{1,1}$, dann können wir parallel dazu auch $u^{k}_{3,1}$, $u^{k}_{2,2}$ und $u^{k}_{1,3}$ berechnen (sowie auch alle weiteren Diagonalen mit je 2 Abstand). Im nächsten Schritt können wir dann alle nun noch nicht abgedeckten Diagonalen gleichzeitig berechnen. In Abbildung \ref{gsv_parallel} sind all diese Diagonalen noch einmal aufgetragen. Die Felder, über die gleichzeitig berechnet werden kann, bilden ein Schachbrettmuster.

Der Grund, weshalb immer eine Diagonale ausgelassen werden muss, liegt in den vorhandenen Datenabhängigkeiten (dieses Mal in die andere Richtung, also zu den Nachbarn rechts und unten). Angenommen, man möchte $u^{k+1}_{1,1}$ und $u^{k}_{2,1}$ parallel berechnen, dann würde man bereits für ersteren den Wert von letzterem benötigen (gemäß Abbildung \ref{gsv_depend}). Lässt man jedoch eine Diagonale frei, beispielhaft zwischen $u^{k+1}_{1,1}$ und $u^{k}_{3,1}$, so konnte man $u^{k}_{2,1}$ bereits vorher berechnen. Im nächsten Schritt (das heißt nach Synchronisierung) kann aus den berechneten $u^{k+1}_{1,1}$ und $u^{k}_{3,1}$ das dazwischen liegende $u^{k+1}_{2,1}$ berechnet werden usw.
	
	
\begin{figure}
	\centering
	\definecolor{cbblack}{RGB}{250,200,200}
	\definecolor{cbwhite}{RGB}{255,255,255}
	\renewcommand*{\arraystretch}{2}
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline
		\cellcolor{cbblack}$u^{k+1}_{1,1}$ & \cellcolor{cbwhite}$u^{k+1}_{2,1}$ & \cellcolor{cbblack}$u^{k}_{3,1}$ & \cellcolor{cbwhite}$u^{k}_{4,1}$ & \cellcolor{cbblack}$u^{k-1}_{5,1}$ & \cellcolor{cbwhite}$u^{k-1}_{6,1}$ & \cellcolor{cbblack}$u^{k-2}_{7,1}$ \\
		\hline
		\cellcolor{cbwhite}$u^{k+1}_{1,2}$ & \cellcolor{cbblack}$u^{k}_{2,2}$ & \cellcolor{cbwhite}$u^{k}_{3,2}$ & \cellcolor{cbblack}$u^{k-1}_{4,2}$ & \cellcolor{cbwhite}$u^{k-1}_{5,2}$ & \cellcolor{cbblack}$u^{k-2}_{6,2}$ & \cellcolor{cbwhite}$u^{k-2}_{7,2}$ \\
		\hline
		\cellcolor{cbblack}$u^{k}_{1,3}$ & \cellcolor{cbwhite}$u^{k}_{2,3}$ & \cellcolor{cbblack}$u^{k-1}_{3,3}$ & \cellcolor{cbwhite}$u^{k-1}_{4,3}$ & \cellcolor{cbblack}$u^{k-2}_{5,3}$ & \cellcolor{cbwhite}$u^{k-2}_{6,3}$ & \cellcolor{cbblack}$u^{k-3}_{7,3}$ \\
		\hline
		\cellcolor{cbwhite}$u^{k}_{1,4}$ & \cellcolor{cbblack}$u^{k-1}_{2,4}$ & \cellcolor{cbwhite}$u^{k-1}_{3,4}$ & \cellcolor{cbblack}$u^{k-2}_{4,4}$ & \cellcolor{cbwhite}$u^{k-2}_{5,4}$ & \cellcolor{cbblack}$u^{k-3}_{6,4}$ & \cellcolor{cbwhite}$u^{k-3}_{7,4}$ \\
		\hline
		\cellcolor{cbblack}$u^{k-1}_{1,5}$ & \cellcolor{cbwhite}$u^{k-1}_{2,5}$ & \cellcolor{cbblack}$u^{k-2}_{3,5}$ & \cellcolor{cbwhite}$u^{k-2}_{4,5}$ & \cellcolor{cbblack}$u^{k-3}_{5,5}$ & \cellcolor{cbwhite}$u^{k-3}_{6,5}$ & \cellcolor{cbblack}$u^{k-4}_{7,5}$ \\
		\hline
		\cellcolor{cbwhite}$u^{k-1}_{1,6}$ & \cellcolor{cbblack}$u^{k-2}_{2,6}$ & \cellcolor{cbwhite}$u^{k-2}_{3,6}$ & \cellcolor{cbblack}$u^{k-3}_{4,6}$ & \cellcolor{cbwhite}$u^{k-3}_{5,6}$ & \cellcolor{cbblack}$u^{k-4}_{6,6}$ & \cellcolor{cbwhite}$u^{k-4}_{7,6}$ \\
		\hline
		\cellcolor{cbblack}$u^{k-2}_{1,7}$ & \cellcolor{cbwhite}$u^{k-2}_{2,7}$ & \cellcolor{cbblack}$u^{k-3}_{3,7}$ & \cellcolor{cbwhite}$u^{k-3}_{4,7}$ & \cellcolor{cbblack}$u^{k-4}_{5,7}$ & \cellcolor{cbwhite}$u^{k-4}_{6,7}$ & \cellcolor{cbblack}$u^{k-5}_{7,7}$ \\
		\hline
	\end{tabular}
	\caption{Verdeutlichung der Vorgehensweise der Parallelisierung. Zuerst werden diejenigen Einträge von $u$ parallel berechnet, die sich in grau Markierten Feldern befinden. Anschließend parallel die Einträge in den weißen Feldern. Es ist zu beachten, dass die Einträge $u_{x,y}$ für unterschiedliche Iterierte $k$ berechnet werden.}
	\label{gsv_parallel}
\end{figure}

Durch diese Schachbrett-artige Parallelisierung hat ein Berechnungsschritt die Gestalt:
\begin{itemize}
	\item Berechne Parallel über alle schwarzen Felder
	\item Berechne Parallel über alle weißen Felder
\end{itemize}
Es sei nicht zu verschweigen, dass hier im Gegensatz zur seriellen Variante sehr wohl mehrere $u$ gleichzeitig zu speichern sind. Sobald in einem Berechnungsschritt das Feld ganz rechts unten, also $u^{k+2-d}_{d,d}$, berechnet wurde, müssen im Falle des Abbruchs (weil das Abbruchkriterium aus \ref{p1a4a} erfüllt ist), alle $u^{k+2-d}_{x,y}$ noch immer bekannt sein, um $u^{k+2-d}$ als Ergebnis ausgeben zu können. Ebenso muss das Abbruchkriterium stets auf den Werten der gleichen Iterierten arbeiten, um exakt das gleiche Resultat wie die serielle Variante zu erhalten.

In jedem Berechnungsschritt wird genaue eine Iterierte des Verfahrens berechnet. Allerdings gilt dies erst, sobald $d-1$ Schritte ausgeführt wurden, denn erst dann ist schließlich der Eintrag rechts unten $u^{k+2-d}_{d,d}=u^{1}_{d,d}$ berechnet (die \emph{Historie} muss erst gefüllt werden). So kommt es, dass die parallele Version bei gleichen Eingaben exakt die gleichen Ergebnisse wie die serielle Version ausgibt, jedoch dabei genau $d-1$ Iterationen mehr benötigt. Wie der Tabelle \ref{gsv_runtime} zu entnehmen, ist die parallele Version mit 32 Kernen bei großer Problemgröße ($l=9$) mit einem \emph{Speedup} von $7,57$ aber immerhin noch deutlich schneller. Die eher geringe Effizienz schreiben wir dem Synchronisationsaufwand sowie den eher Cache-unfreundlichen Speicherzugriffen zu.

\begin{table}
	\centering
	\begin{tabular}{r|r|r||r||r|r||r|r|@{ \dots}|r|r}
		\multicolumn{3}{c||}{Problemgröße} & Seq. & \multicolumn{2}{c||}{2 Threads}  & \multicolumn{2}{c|@{ \dots}|}{4 Threads} & \multicolumn{2}{c}{32 Threads} \\
		$l$ & $d$ & $n$ & Laufzeit & Laufzeit & Speedup & Laufzeit & Speedup & Laufzeit & Speedup \\
		\hline
		4 & 15 & 225 & 0,005 & 0,007 & 0,357 & 0,008 & 0,625 & 1,361 & 0,0389 \\
		5 & 31 & 961 & 0,053 & 0,056 & 0,473 & 0,049 & 1,08 & 4,041 & 0,178 \\
		\hline
		\hline
		8 & 255 & 65.025 & 115 & 81 & 1,42 & 45,3 & 2,54 & 25,33 & 4,54 \\
		9 & 511 & 261.121 & 1340 & 943 & 1,42 & 504 & 2,67 & 177 & 7,57 \\
	\end{tabular}
	\caption{Laufzeiten unserer Parallelisierung gegenüber der sequentiellen Version. Getestet wurde auf dem Rechner \emph{i82pc31}. Um Ungenauigkeiten auszugleichen wurden die Zeiten jeweils über $100$ (für große Eingaben über $10$) Durchläufe gemittelt.}
	\label{gsv_runtime}
\end{table}
	
	

\subsection{Partielle Differentialgleichungen}\label{p1a5}
\subsubsection{}
Die Bedingungen lauten nach den Folien der FEM-Einführung:
\begin{itemize}
	\item $\Omega$ sei ein beschränktes Gebiet\\
	\item $\Gamma$ sei hinreichend glatt\\
	\item $f:\Omega\rightarrow \mathbb{R}$ gegebene Funktion, wie es hier der Fall ist.
\end{itemize}


\subsubsection{}
Gesucht ist $f$ mit
	\begin{equation}\label{udef}
		u(x,y)=\sin(2M\pi x)\sin(2N\pi y)
	\end{equation}
	Dies kann durch einfache Anwendung des \emph{Laplace-Operators $\Delta$} berechnet werden:
	\begin{equation}\label{fgleichung}
	\begin{split}
		f(x,y)&=-\Delta u(x,y) \\
		&= -\frac{\partial u}{\partial x^2}-\frac{\partial u}{\partial y^2} \\
		&= -\frac{\partial}{\partial x}(2M\pi\cos(2M\pi x)\sin(2N\pi y)) -\frac{\partial}{\partial y}(2N\pi\sin(2M\pi x)\cos(2N\pi y)) \\
		&= 4M^2\pi^2\sin(2M\pi x)\sin(2N\pi y) + 4N^2\pi^2\sin(2M\pi x)\sin(2N\pi y) \\
		&= (M^2+N^2)4\pi^2\sin(2M\pi x)\sin(2N\pi y)
	\end{split}
	\end{equation}


\subsubsection{}
Da in Gleichung \ref{fgleichung} $M,N \in\mathbb{N}$ beliebig, wählen wir der Einfachheit halber $M=N=1$ zur Lösung dieser Teilaufgabe. Damit ergibt sich
\begin{equation}\label{fdef}
f(x,y)=8\pi^2\sin(2\pi x)\sin(2\pi y)
\end{equation}
Die dazugehörige analytische Lösung ist dann nach Aufgabenstellung gegeben als
\begin{equation}
u(x,y)=\sin(2\pi x)\sin(2\pi y)
\end{equation}
und wird verwendet um den maximalen Fehler der Näherung zu berechnen.

Das $f$ auf Gleichung \ref{fdef} kann einfach in den Code der vorangegangenen Aufgabe eingesetzt werden. Wie auch \cite{FiniteElemente} entnommen werden kann, lässt sich das Problem mit dem in Aufgabe \ref{p1a4a} gelösten Problem lösen. Es ergeben sich die in Abbildung \ref{grapha5} gezeigten Näherungen. Mit zunehmendem $l$ bzw. abnehmender Gitterkonstante $h$ werden die Lösungen nicht nur feiner, sondern der maximale Fehler zur analytischen Lösung von $u$ auch kleiner wird. Ab $l=8$ wird der Fehler allerdings wieder größer, wie Tabelle \ref{tablea5} entnommen werden kann.
An dieser Stelle scheint das für das Abbruchkriterium gewählte $\varepsilon_{Error}$ aus Aufgabe \ref{p1a4a} zu grob gewählt zu sein, denn eine weitere Verfeinerung dessen reduzierte den Fehler der Näherung (nur) für große $l$ deutlich (was die Frage aufwirft, ob man $\varepsilon_{Error}$ abhängig von $l$ wählen sollte).

\begin{figure}
	\centering
	\vspace{-3em}
	\includegraphics[scale=0.18]{a5l3}
	\includegraphics[scale=0.18]{a5l4}
	\includegraphics[scale=0.18]{a5l5}
	\vspace{-5em}
	\caption{Näherungsweise Lösung für $u$, berechnet mittels Gauß-Seidel-Verfahren. Von links nach rechts: $l=3, l=4, l=5$ bzw. $h=\frac{1}{8}, h=\frac{1}{16}, h=\frac{1}{32}$. Geplottet mittels GNU~Octave.}
	\label{grapha5}
\end{figure}

\begin{table}
	\centering
	\begin{tabular}{r||r|r|r|r|r|r|r|r}
		$l$ & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
		$d$ & 3 & 7 & 15 & 31 & 63 & 127 & 255 & 511 \\
		$h$ & 0,25 & 0,125 & 0,0625 & 0,0313 & 0,0156 & 0,00781 & 0,00391 & 0,00195 \\
		\hline
		Maximaler Fehler & 0,234 & 0,053 & 0,0130 & 0,00326 & 0,000866 & 0,000326 & 0,00174 & 0.0266 \\
		Iterationen & 21 & 64 & 175 & 409 & 1148 & 3515 & 11043 & 98558
	\end{tabular}
	\caption{Maximaler Fehler und Anzahl der durchgeführten Operationen in Abhängigkeit von $l$ bzw. Gitterkonstante $h$.}
	\label{tablea5}
\end{table}

Das verwendete Vorgehen wird als \emph{h-FEM} bezeichnet, da wir lediglich die Gitterkonstante $h$ anpassen. Eine \emph{p-FEM}- oder gar \emph{hp-FEM}-Methodik würde den Grad des zum Abtasten verwendeten Polynoms erhöhen.







\subsection{Partielle Differentialgleichungen}\label{p1a6}
\subsubsection{}\label{p1a6a}
Alphabetische Liste gängiger Krylow-Unterraum-Verfahren (nach \cite{KrylowUnterraumWiki}):
\begin{itemize}
	\item Arnoldi-Verfahren, zur Eigenwertapproximation
	\item BiCG, das CG-Verfahren für nicht SPD-Matrizen
	\item BiCGSTAB, Stabilisierung von CGS
	\item BiCGSTAB(ell), Stabilisierung von CGS
	\item BiCGSTABTFQMR, der Ansatz hinter TFQMR angewandt auf BiCGSTAB
	\item BiOres, eine Variante des BiCG-Verfahrens
	\item BiOmin, eine Variante des BiCG-Verfahrens
	\item BiOdir, eine Variante des BiCG-Verfahrens
	\item CG, zur approximativen Lösung linearer Gleichungssysteme
	\item CGNE, CG-Verfahren auf den Normalgleichungen, Variante 1
	\item CGNR, CG-Verfahren auf den Normalgleichungen, Variante 2
	\item CGS-Verfahren, quadrierte BiCG-Rekursion
	\item FOM, zur approximativen Lösung linearer Gleichungssysteme
	\item GMRES, zur approximativen Lösung linearer Gleichungssysteme
	\item Hessenberg-Verfahren, zur Eigenwertapproximation
	\item Lanczos-Verfahren, zur Eigenwertapproximation
	\item MinRes, zur approximativen Lösung linearer Gleichungssysteme
	\item Orthores, Orthomin und Orthodir, Verallgemeinerungen des CG-Verfahrens
	\item Ores, eine Variante des CG-Verfahrens
	\item Omin, eine Variante des CG-Verfahrens
	\item Odir, eine Variante des CG-Verfahrens
	\item Potenzmethode, älteste Methode zur Eigenwertapproximation
	\item QMR, zur approximativen Lösung linearer Gleichungssysteme
	\item Richardson-Iteration, bei geeigneter Interpretation
	\item SymmLQ, zur approximativen Lösung linearer Gleichungssysteme
	\item TFQMR, zur approximativen Lösung linearer Gleichungssysteme
\end{itemize}


Die häufig verwendeten Grundverfahren sind: CG-Verfahren, GMRES und Lanczos-Verfahren

CG-Verfahren: Hierbei handelt es sich um einen iterativen Algorithmus und ist gedacht für große lineare, symmetrische, positiv definite und dünn besetzte Gleichungssysteme. Das CG-Verfahren liefert nach spätestens n (Dimension der Matrix) Schritten die exakte Lösung.

GMRES: Ein iteratives Verfahren welches sich besonders für große, dünn besetzte lineare Gleichungssysteme eignet. GMRES kann und wird für nicht-symmetrische Matrizen eingesetzt, allerdings wird die exakte Lösung erst nach endlich vielen Schritten geliefert.

Lanczos-Verfahren: Ebenfalls ein iterativer Algorithmus, welcher Eigenwerte mit den entsprechenden Eigenvektoren bestimmen kann, als auch lineare Gleichungssysteme lösen. Die Konvergenz vom Algorithmus ist von den Eigenwerten abhängig.

Bei der Auswahl von diesen drei Verfahren für unser Problem, eignet sich das CG-Verfahren am besten, da die vom Problem gegebene Matrix alle Kriterien erfüllt. Des Weiteren ist die Konvergenz $n$ (nach \cite{Numa}) von Vorteil, da wir davon ausgehen können, dass wir schon viel früher eine Lösung erreicht haben werden welche unter einer gewissen Toleranz \emph{gut genug} sein wird.
Gegen GMRES spricht nur die möglicherweise langsamere Konvergenz und die hier nicht benötigte Flexibilität auch nicht-symmetrische Matrizen bearbeiten zu können.
Das Lanczos-Verfahren wäre eine Alternative, vor allem da die Eigenwerte und somit die Konvergenz im Voraus bekannt sind.

Unsere Parallelisierung arbeitet auf der Ebene der Vektoreinträge, da diese zu großen Teilen unabhängig von einander berechnet werden können (ganz im Gegensatz zum Gauß-Seidel-Verfahren aus \ref{p1a4}). Auch hier kann die dünne Struktur von $A$ dahingehend ausgenutzt werden, dass Berechnungen der Art $A\cdot x$ im Algorithmus für jeden Eintrag im Ergebnisvektor zu vier bedingten Additionen zerfallen. Auch hier muss $A$ daher nicht explizit gespeichert oder berechnet werden.

\TODO{Implementierung soll mit Hilfe der \emph{kennengelernten Werkzeuge} analysiert werden.}

\subsubsection{}\label{p1a6b}
Unsere parallele Version des CG-Verfahrens konnte gegenüber dem Gauß-Seidel-Verfahren das Problem aus Aufgabe \ref{p1a5} bis zu $40$-mal so schnell lösen. Eine Übersicht über den \emph{Speedup} sowie die \emph{Efficiency} ist in Tabelle \ref{a5a6speedupefficiency} zu finden.

\begin{table}
	\centering
	\begin{tabular}{r|r|r||r||r|r|@{ \dots}|r|r||r|r}
		\multicolumn{3}{c||}{Problemgröße} & Seq. & \multicolumn{2}{c|@{ \dots}|}{2 Threads} & \multicolumn{2}{c||}{16 Threads}  & \multicolumn{2}{c}{32 Threads} \\
		$l$ & $d$ & $n$ & Speedup & Speedup & Efficiency & Speedup & Efficiency & Speedup & Efficiency \\
		\hline
		4 & 15 & 225 & 1,74 & 1,81 & 0,906 & 2,44 & 0,153 & 2,93 & 0,0917 \\
		5 & 31 & 961 & 4,01 & 3,71 & 1,86 & 1,44 & 0,0898 & 3,46 & 0,109 \\
		\hline
		\hline
		8 & 255 & 65.025 & 17,9 & 16,5 & 8,27 & 16,8 & 1,05 & 4,05 & 0,127 \\
		9 & 511 & 261.121 & 27,6 & 25,9 & 12,9 & 39,5 & 2,47 & 37,9 & 1,18
	\end{tabular}
	\caption{\emph{Speedup} und \emph{Efficiency} für das CG-Verfahren gegenüber dem Gauß-Seidel-Verfahren für das Problem aus Aufgabe \ref{p1a5}. Die zugrunde liegenden Laufzeiten wurden anhand des Rechners \emph{i82pc31} ermittelt.}
	\label{a5a6speedupefficiency}
\end{table}

Wie auch unsere Implementierung des Gauß-Seidel-Verfahrens aus Aufgabe \ref{p1a4}, nutzen wir hier für unsere Implementierung die dünne Struktur der Matrix $A$ aus. Eine Vorkonditionierung würde diese Struktur beeinträchtigen, wodurch ein sehr viel höherer Rechenaufwand zu erwarten wäre. Gesetzt den Fall unsere Matrix $A$ wäre nicht dünn besetzt, so könnte eine Vorkonditionierung Sinn machen, um die Stabilität des Algorithmus ggf. zu erhöhen.



\subsubsection{}\label{p1a6c}
In Tabelle \ref{MathBibs} haben wir drei Bibliotheken zur Anwendung der Finiten Elemente Methode aufgelistet und verglichen. Weitere Bibliotheken, die es nicht in die Tabelle geschafft haben, wären beispielsweise Feel++\cite{Feel++} und
Libmesh\cite{Libmesh}.

Im Rahmen dieses Praktikums würde sich, zumindest für Projekt 1, \emph{HiFlow\textsuperscript{3}} gut eignen, da hier eine ausgereifte OpenMP Funktionalität gegeben zu sein scheint, die den Portierungsaufwand stark verringert.

\begin{table}
	\centering
	\begin{tabular}{l|p{0.6\textwidth}|l}
		Bibliothek & Unterschiede & Gemeinsamkeiten \\
		\hline
		HiFlow\textsuperscript{3}\cite{HiFlow3} &
		MPI und OpenMP basiert, \\
		&Hohe Parallelität (von Laptop bis zu Cluster), \\
		&Keine externen Bibliotheken benötigt (optional Libraries, unter anderem OpenMP)
		& \multirow{9}{0.3\textwidth}{Für C++ gedacht, \newline
			Implementieren Krylov-Unterraumverfahren,
			Diskretisierung wählbar} \\
		
		\cline{1-2}
		MFEM\cite{MFEM} &
		MPI basiert (und experimenteller OpenMP Support), \\
		&Sehr hohe Parallelität (mehrere hunderttausend Kerne), \\
		&Keine externen Bibliotheken benötigt \\
		
		\cline{1-2}
		Deal.II\cite{Deal.II} &
		MPI basiert, \\
		&Hohe Parallelität (16000 Kerne mindestens), \\
		&Keine externen Bibliotheken benötigt (optional Libraries)
	\end{tabular}
	\caption{Ausgewählte Bibliotheken zur Anwendung der Finiten Elemente Methode im Vergleich.}
	\label{MathBibs}
\end{table}


\section{}

% Normaler LNCS Zitierstil
%\bibliographystyle{splncs}
%\bibliographystyle{itmalpha}
\bibliographystyle{natdin}
% TODO: Ändern der folgenden Zeile, damit die .bib-Datei gefunden wird
\bibliography{literatur}

\end{document}

